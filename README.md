[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a2j-anchor-to-joint-regression-network-for-3d/pose-estimation-on-hands-2017)](https://paperswithcode.com/sota/pose-estimation-on-hands-2017?p=a2j-anchor-to-joint-regression-network-for-3d) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a2j-anchor-to-joint-regression-network-for-3d/hand-pose-estimation-on-nyu-hands)](https://paperswithcode.com/sota/hand-pose-estimation-on-nyu-hands?p=a2j-anchor-to-joint-regression-network-for-3d)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a2j-anchor-to-joint-regression-network-for-3d/hand-pose-estimation-on-icvl-hands)](https://paperswithcode.com/sota/hand-pose-estimation-on-icvl-hands?p=a2j-anchor-to-joint-regression-network-for-3d) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a2j-anchor-to-joint-regression-network-for-3d/pose-estimation-on-itop-front-view)](https://paperswithcode.com/sota/pose-estimation-on-itop-front-view?p=a2j-anchor-to-joint-regression-network-for-3d)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/a2j-anchor-to-joint-regression-network-for-3d/pose-estimation-on-itop-top-view)](https://paperswithcode.com/sota/pose-estimation-on-itop-top-view?p=a2j-anchor-to-joint-regression-network-for-3d)



# A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image
## Introduction
This is the official implementation for the paper, **"A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image"**, ICCV 2019. 

In this paper, we propose a simple and effective approach termed A2J, for 3D hand and human pose estimation from a single depth image. Wide-range evaluations on 5 datasets demonstrate A2J's superiority.

Please refer to our paper for more details, https://arxiv.org/abs/1908.09999.

![pipeline](https://github.com/zhangboshen/A2J/blob/master/fig/A2Jpipeline.png)

If you find our work useful in your research or publication, please cite our work:
```
@inproceedings{A2J,
author = {Xiong, Fu and Zhang, Boshen and Xiao, Yang and Cao, Zhiguo and Yu, Taidong and Zhou Tianyi, Joey and Yuan, Junsong},
title = {A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation from a Single Depth Image},
booktitle = {Proceedings of the IEEE Conference on International Conference on Computer Vision (ICCV)},
year = {2019}
}
```
## Comparison with state-of-the-art methods
![result_hand](https://github.com/zhangboshen/A2J/blob/master/fig/result_hand.png)
![result_body](https://github.com/zhangboshen/A2J/blob/master/fig/result_body.png)


## Qualitative Results
#### [NYU](https://jonathantompson.github.io/NYU_Hand_Pose_Dataset.htm) hand pose dataset:
![NYU_1](https://github.com/zhangboshen/A2J/blob/master/fig/NYU_1.png)
&nbsp;

#### [ITOP](https://www.alberthaque.com/projects/viewpoint_3d_pose/) body pose dataset:
![ITOP_1](https://github.com/zhangboshen/A2J/blob/master/fig/ITOP_1.png)


# Code is coming soon...


